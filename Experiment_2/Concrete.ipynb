{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xaBWVbeFDwn"
   },
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras_flops import get_flops\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import random as random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for the GPU usage\n",
    "import tensorflow.keras.backend as KB\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config = config)\n",
    "tf.compat.v1.keras.backend.set_session(session)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxeaqNRqgxza"
   },
   "outputs": [],
   "source": [
    "## Internal Parameters (set according to Table 1)\n",
    "\n",
    "# NAS\n",
    "K = [5,5] # Number of stages, (5 = Convolution layers, and 2 = Input and Output)\n",
    "nDense = 5 # Number of dense FC leayers\n",
    "\n",
    "Space_Filters = [2**i for i in range(1, 8)]\n",
    "Space_Kernels = [2*i+1 for i in range(1, 4)]\n",
    "Space_Activations= ['linear','tanh','sigmoid','relu','leaky_relu'] \n",
    "Space_Maxpooling = [2*i for i in range(1, 3)]\n",
    "Space_Neurons = [2*i for i in range(3, 25)]\n",
    "Space_Neurons.insert(0,0)\n",
    "Space_Optimizers = ['RMSprop', 'Adam', 'Adagrad', 'Adadelta']\n",
    "Space_lr = [10**(-i) for i in range(2,5)]\n",
    "\n",
    "# QNSA\n",
    "nIter = 40 # Number of iterations\n",
    "nPop = 10 # Number of population\n",
    "nEpoch = 100 # Number of Epochs\n",
    "pCrossover = 0.9                         # Crossover Percentage\n",
    "pMutation = 0.1;                          # Mutation Percentage\n",
    "mu = 0.1;                    # Mutation Rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "nClass = 3 # Number of classes\n",
    "INPUTSHAPE = (200,200,3) # Shape of  input images\n",
    "imgtype = 'rgb' # Type of image\n",
    "\n",
    "pic_folder_train = \"content/train\"\n",
    "pic_folder_valid = \"content/valid\"\n",
    "pic_folder_test = \"content/test\"\n",
    "\n",
    "# Net parameters\n",
    "RANDOMROT = 0.5 # 0 means NO\n",
    "RANDOMFLIP = \"horizontal_and_vertical\" # 0 means NO\n",
    "\n",
    "runnum = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_tZkaB10xPB"
   },
   "outputs": [],
   "source": [
    "# Making Folders\n",
    "\n",
    "!mkdir HistoryArchive\n",
    "!mkdir ModelArchive\n",
    "!mkdir TrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for importing Dataset\n",
    "\n",
    "def get_all_path(base_path):\n",
    "\n",
    "    folder_cr = os.path.join(base_path, 'cr')\n",
    "    folder_sp = os.path.join(base_path, 'sp')\n",
    "    folder_nd = os.path.join(base_path, 'nd')\n",
    "    \n",
    "    img_cr_fname = os.listdir(folder_cr)\n",
    "    img_sp_fname = os.listdir(folder_sp)\n",
    "    img_nd_fname = os.listdir(folder_nd)\n",
    "    \n",
    "    all_images_paths = []\n",
    "    for i in range(len(img_cr_fname)):\n",
    "        all_images_paths.append(os.path.join(folder_cr, img_cr_fname[i]))\n",
    "    for i in range(len(img_sp_fname)):\n",
    "        all_images_paths.append(os.path.join(folder_sp, img_sp_fname[i]))\n",
    "    for i in range(len(img_nd_fname)):\n",
    "        all_images_paths.append(os.path.join(folder_nd, img_nd_fname[i]))\n",
    "        \n",
    "    label_name = ['cr','sp','nd']\n",
    "    label_to_index = dict((name, index) for index, name in enumerate(label_name))\n",
    "\n",
    "    all_images_labels = [label_to_index[pathlib.Path(path).parent.name] for path in all_images_paths]\n",
    "    \n",
    "    return all_images_paths, all_images_labels\n",
    "\n",
    "\n",
    "def creat_dataset(pic_folder_train, pic_folder_test, pic_folder_valid, target_size, img_mode):\n",
    "    \n",
    "    train_images_paths, train_images_labels = get_all_path(pic_folder_train) \n",
    "    test_images_paths, test_images_labels = get_all_path(pic_folder_test)\n",
    "    valid_images_paths, valid_images_labels = get_all_path(pic_folder_valid)\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    x_valid = []\n",
    "    y_valid = []\n",
    "    \n",
    "    if img_mode == 'gray':\n",
    "        for n in range(len(train_images_paths)):\n",
    "            image_path = train_images_paths[n]\n",
    "            image_label = train_images_labels[n]\n",
    "            img = cv2.imread(image_path, flags = 0)\n",
    "            img = cv2.resize(img, target_size)\n",
    "            x_train.append(img)\n",
    "            y_train.append(image_label)\n",
    "\n",
    "        for n in range(len(test_images_paths)):\n",
    "            image_path = test_images_paths[n]\n",
    "            image_label = test_images_labels[n]\n",
    "            img = cv2.imread(image_path, flags = 0)\n",
    "            img = cv2.resize(img, target_size)\n",
    "            x_test.append(img)\n",
    "            y_test.append(image_label)\n",
    "        for n in range(len(valid_images_paths)):\n",
    "            image_path = valid_images_paths[n]\n",
    "            image_label = valid_images_labels[n]\n",
    "            img = cv2.imread(image_path, flags = 0)\n",
    "            img = cv2.resize(img, target_size)\n",
    "            x_valid.append(img)\n",
    "            y_valid.append(image_label)\n",
    "    else:\n",
    "        for n in range(len(train_images_paths)):\n",
    "            image_path = train_images_paths[n]\n",
    "            image_label = train_images_labels[n]\n",
    "            img = cv2.imread(image_path)\n",
    "#             img = cv2.resize(img, target_size)\n",
    "            img = cv2.resize(img, target_size)/255\n",
    "            x_train.append(img)\n",
    "            y_train.append(image_label)\n",
    "\n",
    "        for n in range(len(test_images_paths)):\n",
    "            image_path = test_images_paths[n]\n",
    "            image_label = test_images_labels[n]\n",
    "            img = cv2.imread(image_path)\n",
    "#             img = cv2.resize(img, target_size)\n",
    "            img = cv2.resize(img, target_size)/255\n",
    "            x_test.append(img)\n",
    "            y_test.append(image_label)\n",
    "            \n",
    "        for n in range(len(valid_images_paths)):\n",
    "            image_path = valid_images_paths[n]\n",
    "            image_label = valid_images_labels[n]\n",
    "            img = cv2.imread(image_path)\n",
    "#             img = cv2.resize(img, target_size)\n",
    "            img = cv2.resize(img, target_size)/255\n",
    "            x_valid.append(img)\n",
    "            y_valid.append(image_label)\n",
    "            \n",
    "    x_train = np.array(x_train)\n",
    "    if img_mode == 'gray':\n",
    "        x_train = x_train.reshape(x_train.shape[0], target_size[0], target_size[1], 1).astype('float32')/255.\n",
    "        \n",
    "    x_test = np.array(x_test)\n",
    "    if img_mode == 'gray':\n",
    "        x_test = x_test.reshape(x_test.shape[0], target_size[0], target_size[1], 1).astype('float32')/255.\n",
    "        \n",
    "    x_valid = np.array(x_valid)\n",
    "    if img_mode == 'gray':\n",
    "        x_valid = x_valid.reshape(x_valid.shape[0], target_size[0], target_size[1], 1).astype('float32')/255.\n",
    "\n",
    "\n",
    "    y_train = np.array(y_train)\n",
    "    y_train = tf.keras.utils.to_categorical(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test)\n",
    "    y_vaid = np.array(y_valid)\n",
    "    y_valid = tf.keras.utils.to_categorical(y_valid)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test), (x_valid, y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmwrG6V1v2XJ",
    "outputId": "dae612f9-b617-4eae-8778-66d1cfedb0cc"
   },
   "outputs": [],
   "source": [
    "# Importing dataset\n",
    "(x_train, y_train), (x_test, y_test), (x_valid, y_valid) = creat_dataset(pic_folder_train, pic_folder_test,pic_folder_valid, INPUTSHAPE[:-1], imgtype)\n",
    "dataset = ((x_train, y_train), (x_test, y_test), (x_valid, y_valid))\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_shuffled,y_shuffled = shuffle(x_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range = 90, horizontal_flip = True, vertical_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yiBQwy12h9mJ",
    "outputId": "3b156802-3854-465e-810f-d5d8f81587cb"
   },
   "outputs": [],
   "source": [
    "# Number of bits\n",
    "\n",
    "nBits_Arch_list = [0.5*kk*(kk-1) for kk in K]\n",
    "nBits_Arch = np.sum(nBits_Arch_list)\n",
    "nBits_Arch = int(nBits_Arch)\n",
    "\n",
    "nBits_Filters = np.sum(K)+ 2*len(K)\n",
    "nBits_Filters = int(nBits_Filters)\n",
    "\n",
    "nBits_Kernels = nBits_Filters \n",
    "nBits_Kernels = int(nBits_Kernels)\n",
    "\n",
    "nBits_MaxP = len(K)\n",
    "nBits_MaxP = int(nBits_MaxP)\n",
    "\n",
    "nBits_Neuron = nDense\n",
    "nBits_Neuron =int(nBits_Neuron)\n",
    "\n",
    "nBits_Acts = nBits_Filters + nBits_Neuron # one for dense layers\n",
    "nBits_Acts = int(nBits_Acts)\n",
    "\n",
    "nBits_Optimizer = 1\n",
    "nBits_lr = 1\n",
    "\n",
    "nBits = nBits_Arch + nBits_Filters + nBits_Kernels + nBits_Acts + nBits_MaxP + nBits_Neuron + nBits_Optimizer + nBits_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-yKy5DYnHvj"
   },
   "outputs": [],
   "source": [
    "# list of optimizing variables\n",
    "\n",
    "BITS_list = [nBits_Arch,nBits_Filters,nBits_Kernels,nBits_Acts,nBits_MaxP,nBits_Neuron,nBits_Optimizer,nBits_lr]\n",
    "SPACE_list = [Space_Filters, Space_Kernels, Space_Activations, Space_Maxpooling, Space_Neurons, Space_Optimizers,Space_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of Parameters\n",
    "\n",
    "Type_List = np.zeros(nBits)\n",
    "endd = nBits_Arch\n",
    "Type_List[0: nBits_Arch] = 0 # Binary\n",
    "Type_List[nBits_Arch:nBits_Arch+ nBits_Filters+ nBits_Kernels] = 1 # Discrete\n",
    "Type_List[nBits_Arch+nBits_Filters+nBits_Kernels:nBits_Arch+ nBits_Filters+nBits_Kernels+ nBits_Acts] = 2 # Category\n",
    "Type_List[nBits_Arch+nBits_Filters+nBits_Kernels+nBits_Acts:nBits_Arch+ nBits_Filters+nBits_Kernels+ nBits_Acts+nBits_MaxP+nBits_Neuron] = 1 # Discrete\n",
    "Type_List[nBits_Arch+ nBits_Filters+nBits_Kernels+ nBits_Acts+nBits_MaxP+nBits_Neuron:nBits_Arch+ nBits_Filters+nBits_Kernels+ nBits_Acts+nBits_MaxP+nBits_Neuron+nBits_Neuron+nBits_Optimizer] = 2 # Category\n",
    "Type_List[nBits_Arch+ nBits_Filters+nBits_Kernels+ nBits_Acts+nBits_MaxP+nBits_Neuron+nBits_Optimizer:nBits_Arch+ nBits_Filters+nBits_Kernels+ nBits_Acts+nBits_MaxP+nBits_Neuron+nBits_Neuron+nBits_Optimizer+nBits_lr] = 1 # Discrete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rf64YQLRnxoa"
   },
   "outputs": [],
   "source": [
    "# Upper and lower bounds\n",
    "\n",
    "Lower = np.zeros((1, nBits))\n",
    "Lower = Lower.reshape(-1,1)\n",
    "\n",
    "Upper = [np.ones((1, nBits_Arch))+1,\n",
    "          len(Space_Filters)*np.ones((1, nBits_Filters)),\n",
    "          len(Space_Kernels)*np.ones((1, nBits_Kernels)),\n",
    "          len(Space_Activations)*np.ones((1, nBits_Acts)),\n",
    "          len(Space_Maxpooling)*np.ones((1, nBits_MaxP)),\n",
    "          len(Space_Neurons)*np.ones((1, nBits_Neuron)),\n",
    "          len(Space_Optimizers)*np.ones((1, nBits_Optimizer)),\n",
    "          len(Space_lr)*np.ones((1, nBits_lr))]\n",
    "Upper = np.concatenate( Upper, axis=1)\n",
    "Upper = Upper [0].reshape(-1,1)-1\n",
    "\n",
    "bounds = np.stack((Lower.T, Upper.T), axis=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caluclating the number of models generated by combination and mutation operators\n",
    "\n",
    "nCrossover = 2*np.round(pCrossover*nPop/2)  # Number of Parnets (Offsprings)\n",
    "nMutation = np.round(pMutation*nPop)        # Number of Mutants\n",
    "\n",
    "sigma = 0.1* (Upper - Lower) #np.ones((1,nBits))  # Mutation Step Size\n",
    "sigma = sigma.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eh9izaAfLpnc"
   },
   "outputs": [],
   "source": [
    "# Converting Bits to lists\n",
    "\n",
    "def Bit2LISTs(sol,BITS_list,SPACE_list, nClass, K):\n",
    "  \n",
    "  nBits_Arch = BITS_list[0]\n",
    "  nBits_Filters = BITS_list[1]\n",
    "  nBits_Kernels = BITS_list[2]\n",
    "  nBits_Acts = BITS_list[3]\n",
    "  nBits_MaxP = BITS_list[4]\n",
    "  nBits_Neuron = BITS_list[5]\n",
    "  nBits_Optimizer = BITS_list[6]\n",
    "  nBits_lr = BITS_list[7]\n",
    "\n",
    "  Space_Filters = SPACE_list[0]\n",
    "  Space_Kernels = SPACE_list[1]\n",
    "  Space_Activations = SPACE_list[2]\n",
    "  Space_Maxpooling = SPACE_list[3]\n",
    "  Space_Neurons = SPACE_list[4]\n",
    "  Space_Optimizers = SPACE_list[5]\n",
    "  Space_lr = SPACE_list[6]\n",
    "\n",
    "  AR = nBits_Arch\n",
    "  Filters = sol[AR:AR+nBits_Filters]\n",
    "  F = np.array(Space_Filters)[Filters.astype(int)]\n",
    "\n",
    "\n",
    "  NK = AR+nBits_Filters\n",
    "  Kernels = sol[NK:NK+nBits_Kernels]\n",
    "  KR = np.array(Space_Kernels)[Kernels.astype(int)]\n",
    "\n",
    "  NA = NK + nBits_Kernels\n",
    "  Acts = sol[NA:NA+nBits_Acts]\n",
    "  A = np.array(Space_Activations)[Acts.astype(int)]\n",
    "  NM = NA+nBits_Acts\n",
    "  MaxP = sol[NM:NM+nBits_MaxP]\n",
    "  M = np.array(Space_Maxpooling)[MaxP.astype(int)]\n",
    "\n",
    "  NN = NM+nBits_MaxP\n",
    "  Neuron = sol[NN:NN+nBits_Neuron]\n",
    "  N = np.array(Space_Neurons)[Neuron.astype(int)]\n",
    "  N[-1] = nClass\n",
    "\n",
    "  NO = NN+nBits_Neuron\n",
    "  Optimizer = sol[NO:NO+nBits_Optimizer]\n",
    "  O = list(np.array(Space_Optimizers)[Optimizer.astype(int)])\n",
    "\n",
    "  Nlr = NO+nBits_Optimizer\n",
    "  LearningRate = sol[Nlr:Nlr+nBits_lr]\n",
    "  LR = list(np.array(Space_lr)[LearningRate.astype(int)])\n",
    "\n",
    "# Split into different stages\n",
    "  endF = 0\n",
    "  endk = 0\n",
    "  endA = 0\n",
    "\n",
    "  FF = []\n",
    "  KK = []\n",
    "  AA = []\n",
    "\n",
    "  c = -1\n",
    "  for j in K:\n",
    "    c += 1\n",
    "    # Filters\n",
    "    startF = endF \n",
    "    endF = startF + j + 2\n",
    "    FF.append (F[startF:endF])\n",
    "\n",
    "    # Kernels\n",
    "    startK = endk\n",
    "    endk = startK + j + 2\n",
    "    KK.append (KR[startF:endF])\n",
    "\n",
    "    # Activations\n",
    "    startA = endA\n",
    "    endA = startA + j + 2\n",
    "    if c == len(K)-1:\n",
    "      endA = endA + nBits_Neuron\n",
    "    AA.append (list(A[startA:endA]))\n",
    "\n",
    "  AA[-1][-1] ='softmax'\n",
    "  LISTs = [FF,KK,M,N,AA,O,LR]\n",
    "  \n",
    "  return LISTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bw6bF456zJXW"
   },
   "outputs": [],
   "source": [
    "# Architecture connector and repair\n",
    "\n",
    "def Bit2SOLMat(K,sol):\n",
    "  # Number of stages\n",
    "  nStage = len(K)\n",
    "  # Separate the architecture\n",
    "  nBits_Arch_list = [int(0.5*kk*(kk-1)) for kk in K]\n",
    "  # print(nBits_Arch_list)\n",
    "  MAT = [None] * len(K)\n",
    "  start = 0\n",
    "  end = 0\n",
    "  c = -1\n",
    "  for i in nBits_Arch_list:\n",
    "    c += 1\n",
    "    end += i\n",
    "\n",
    "    arch_i = sol[start:end]\n",
    "    start = end\n",
    "\n",
    "  # Bit to Mat\n",
    "    nNode =  K[c]\n",
    "    mat = np.zeros((nNode+2,nNode+2))\n",
    "\n",
    "    Index = np.tril_indices(nNode,k=-1)\n",
    "\n",
    "    In1 = Index [0] + 1\n",
    "    In2 = Index [1] + 1\n",
    "    NewIndex = (In1,In2)\n",
    "\n",
    "    mat[NewIndex] = arch_i\n",
    "\n",
    "    # Connect\n",
    "    # if isolated save to next step\n",
    "    Inps = np.sum(mat,axis = 1)\n",
    "    Outs = np.sum(mat,axis = 0)\n",
    "    SumInpOut = Inps + Outs\n",
    "    Iso = np.where(SumInpOut == 0)[0]\n",
    "    Isolated = Iso[1:-1] # Except first and last nodes\n",
    "    \n",
    "    if (len(Iso) == nNode+2):\n",
    "      mat[-1,0] = 1\n",
    "    else:\n",
    "      # sum  horizontal if zero and not isolated (setdiff) → connect to the first node\n",
    "      NoInp = np.where(Inps == 0)[0]\n",
    "      NoInp_ = np.setdiff1d(NoInp, Iso)\n",
    "      mat[NoInp_,0] = 1\n",
    "\n",
    "      # sum  vertical if zero  and not isolated (setdiff) → connect to the last node\n",
    "      NoOut = np.where(Outs == 0)[0]\n",
    "      NoOut_ = np.setdiff1d(NoOut, Iso)\n",
    "      mat[-1,NoOut_] = 1\n",
    "\n",
    "    MAT[c] = mat\n",
    "  return MAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCSRYZwAFHJ3"
   },
   "outputs": [],
   "source": [
    "# Generating Deep CNN Architectures (Decoding)\n",
    "\n",
    "def CNNGenerator(SOLMAT,ACTLIST,FILTERLIST,KERNELIST,K,InputShape,ModelName,MAXPLIST,nNeurons,RANDOMROT,RANDOMFLIP):\n",
    "    # Definition of connections:\n",
    "    # Each layer command presents a connection that enters a computation node and then exits.\n",
    "    # 1) X_i_j comes from node i and enters node j.\n",
    "    # 2) X_i is the gathering of all connections enering node i; therfore, if we have only one connection to node i, we'll not have this conncetion.\n",
    "    # 3) X_Input is the first connection  of network.\n",
    "    # 4) X_Output is the last connection of stages.\n",
    "\n",
    "  s = -1\n",
    "  for kk in K: # Each stage\n",
    "\n",
    "    s += 1 # stage\n",
    "    print('---------- STAGE = ', s,' ----------')\n",
    "    solmat = SOLMAT[s]\n",
    "    ActList = ACTLIST[s]\n",
    "    FilterList  = FILTERLIST[s]\n",
    "    KerneList = KERNELIST[s]\n",
    "\n",
    "    if s == 0:\n",
    "      exec(\"X_Input = tf.keras.Input(shape=\"+str(INPUTSHAPE)+\", name='img')\")\n",
    "      print(\"X_Input = tf.keras.Input(shape=\"+str(INPUTSHAPE)+\", name='img')\")\n",
    "\n",
    "    # Isolated nodes\n",
    "    Inps = np.sum(solmat,axis = 1)\n",
    "    Outs = np.sum(solmat,axis = 0)\n",
    "    SumInpOut = Inps + Outs\n",
    "    Isolated = np.where(SumInpOut == 0)[0]\n",
    "\n",
    "    # CNN generation\n",
    "\n",
    "    print('solmat = ')\n",
    "    print(solmat)\n",
    "    for i in np.arange(0,1+K[s]+1):\n",
    "      # Decline isolated nodes\n",
    "      if i in Isolated:\n",
    "        continue\n",
    "\n",
    "      listofouts =  np.where(solmat[:,i] == 1)[0]\n",
    "      listofins =  np.where(solmat[i,:] == 1)[0]\n",
    "      outed = []\n",
    "      if i == 0:     # X_0 is the input, so we have one input\n",
    "        for j in listofouts:\n",
    "          if s == 0:\n",
    "            string0 = \"X_0_\"+str(j)+\" = layers.Conv2D(\"+str(FilterList[i])+\", \"+str(KerneList[i])+\", activation='\"+ActList[i]+\"', padding='same')(X_Input)\"\n",
    "            string = \"X_0_\"+str(j)+\" = (\"+prev+\")\" if i in outed else string0\n",
    "            outed.append(0)\n",
    "            prev = \"X_0_\"+str(j)\n",
    "            print(string)\n",
    "            exec(string)\n",
    "          else:\n",
    "            string0 = \"X_0_\"+str(j)+\" = layers.Conv2D(\"+str(FilterList[i])+\", \"+str(KerneList[i])+\", activation='\"+ActList[i]+\"', padding='same')(X_Output)\"\n",
    "            string = \"X_0_\"+str(j)+\" = (\"+prev+\")\" if i in outed else string0\n",
    "            outed.append(0)\n",
    "            prev = \"X_0_\"+str(j)\n",
    "            print(string)\n",
    "            exec(string)\n",
    "\n",
    "      elif len(listofins)==1: ## if one input, but not the input layer -----------------------------------\n",
    "        stringInput = str(listofins[0])\n",
    "        if i == K[s]+1: # Last node of the stage\n",
    "            string = \"X_Output = layers.Conv2D(\"+str(FilterList[i])+\", \"+str(KerneList[i])+\", activation='\"+ActList[i]+\"', padding='same')(X_\"+stringInput+\"_\"+str(i)+\")\"\n",
    "            print(string)\n",
    "            exec(string)\n",
    "        else: # not Last node\n",
    "          for j in listofouts:\n",
    "          \n",
    "            string0 = \"X_\"+str(i)+\"_\"+str(j)+\" = layers.Conv2D(\"+str(FilterList[i])+\", \"+str(KerneList[i])+\", activation='\"+ActList[i]+\"', padding='same')(X_\"+stringInput+\"_\"+str(i)+\")\"\n",
    "            string = \"X_\"+str(i)+\"_\"+str(j)+\" = (\"+prev+\")\" if i in outed else string0\n",
    "            prev = \"X_\"+str(i)+\"_\"+str(j)\n",
    "            outed.append(i)\n",
    "            print(string)\n",
    "            exec(string)\n",
    "\n",
    "      elif len(listofins)==2: ## if two inputs ----------------------------------\n",
    "        # Input gathering\n",
    "\n",
    "        gatheringstring = \"X_\"+str(i)+\" = layers.Concatenate()([X_\"+str(listofins[0])+\"_\"+str(i)+\", X_\"+str(listofins[1])+\"_\"+str(i)+\"])\"\n",
    "        print(gatheringstring)\n",
    "        exec(gatheringstring)\n",
    "\n",
    "        # Output calculation\n",
    "        stringInput = \"X_\"+str(i)\n",
    "        if i == K[s]+1: # Last node of stage\n",
    "            string = \"X_Output = layers.Conv2D(\"+str(FilterList[i])+\", \"+str(KerneList[i])+\", activation='\"+ActList[i]+\"', padding='same')(\"+stringInput+\")\"\n",
    "            print(string)\n",
    "            exec(string)\n",
    "        else: # not Last node\n",
    "          for j in listofouts:\n",
    "\n",
    "            string0 = \"X_\"+str(i)+\"_\"+str(j)+\" = layers.Conv2D(\"+str(FilterList[i])+\", \"+str(KerneList[i])+\", activation='\"+ActList[i]+\"', padding='same')(\"+stringInput+\")\"\n",
    "            string = \"X_\"+str(i)+\"_\"+str(j)+\" = (\"+prev+\")\" if i in outed else string0\n",
    "            prev = \"X_\"+str(i)+\"_\"+str(j)\n",
    "            outed.append(i)\n",
    "            print(string)\n",
    "            exec(string)\n",
    "\n",
    "      else: ## if more than two inputs ------------------------------------------\n",
    "        # Input gathering\n",
    "        \n",
    "        gatheringstring = \"X_\"+str(i)+\" = layers.Concatenate()([X_\"+str(listofins[0])+\"_\"+str(i)+\", X_\"+str(listofins[1])+\"_\"+str(i)+\"])\"\n",
    "        print(gatheringstring)\n",
    "        exec(gatheringstring)\n",
    "\n",
    "        for k in listofins[2:]:\n",
    "          gatheringstring = \"X_\"+str(i)+\" = layers.Concatenate()([X_\"+str(k)+\"_\"+str(i)+\", X_\"+str(i)+\"])\" \n",
    "          print(gatheringstring)\n",
    "          exec(gatheringstring)\n",
    "\n",
    "        # Outout calculation\n",
    "        stringInput = \"X_\"+str(i)\n",
    "        if i == K[s]+1: # Last node of stage\n",
    "            string = \"X_Output = layers.Conv2D(\"+str(FilterList[i])+\", \"+str(KerneList[i])+\", activation='\"+ActList[i]+\"', padding='same')(\"+stringInput+\")\"\n",
    "            print(string)\n",
    "            exec(string)\n",
    "        else: # not Last node\n",
    "          for j in listofouts:\n",
    "            string0 = \"X_\"+str(i)+\"_\"+str(j)+\" = layers.Conv2D(\"+str(FilterList[i])+\", \"+str(KerneList[i])+\", activation='\"+ActList[i]+\"', padding='same')(\"+stringInput+\")\"\n",
    "            string = \"X_\"+str(i)+\"_\"+str(j)+\" = (\"+prev+\")\" if i in outed else string0\n",
    "            prev = \"X_\"+str(i)+\"_\"+str(j)\n",
    "            outed.append(i)\n",
    "            print(string)\n",
    "            exec(string)\n",
    "\n",
    "    print('---------- MaxPooling ----------')\n",
    "    # Maxpooling\n",
    "    strMaxpooling = \"X_Output = layers.MaxPooling2D(\"+str(MAXPLIST[s])+\")(X_Output)\"\n",
    "    exec(strMaxpooling)\n",
    "    print(strMaxpooling)\n",
    "\n",
    "  print('---------- Dense ----------')\n",
    "  # Ending Flatten \n",
    "  strBN = \"X_Output = layers.BatchNormalization()(X_Output)\"\n",
    "  exec(strBN)\n",
    "  print(strBN)\n",
    "\n",
    "  strFlatten= 'X_Output2 = layers.Flatten()(X_Output)'\n",
    "  exec(strFlatten)\n",
    "  print(strFlatten)\n",
    "\n",
    "  # Ending Dense\n",
    "  for m in np.arange(0,len(nNeurons)):\n",
    "    i = i + 1\n",
    "    if nNeurons[m]!=0 :\n",
    "      nNeuron = nNeurons[m]\n",
    "      strDense = \"X_Output2 = layers.Dense(\"+str(nNeuron)+\", activation = '\"+ActList[i]+\"')(X_Output2)\"\n",
    "      exec(strDense)\n",
    "      print(strDense)\n",
    "\n",
    "  exec(\"model2 = tf.keras.Model(X_Input, X_Output2, name='my_net')\")\n",
    "  exec(\"model2.save('ModelArchive/\"+ ModelName +\".h5')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing extra files from previous runs\n",
    "\n",
    "def RemoveFiles(mypath):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "    for f in onlyfiles:\n",
    "        os.remove(os.path.join(mypath, f))\n",
    "\n",
    "RemoveFiles('ModelArchive')\n",
    "RemoveFiles('TrainedModel')\n",
    "RemoveFiles('HistoryArchive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the model has already been saved\n",
    "\n",
    "def CheckAndCost(mysolution,newsol,ModelName):\n",
    "\n",
    "    cost = 'NotFound'\n",
    "    \n",
    "    if not mysolution:\n",
    "        return cost\n",
    "    \n",
    "    for sol in mysolution:\n",
    "        if np.sum(np.abs(newsol - sol[3]))==0:\n",
    "            cost = sol[4]\n",
    "            name = sol[0]\n",
    "            \n",
    "            srcpath1 = \"HistoryArchive/\"+name+\".npy\"\n",
    "            dstpath1 = \"HistoryArchive/\"+ModelName+\".npy\"\n",
    "            shutil.copy(srcpath1,dstpath1)\n",
    "            \n",
    "            srcpath1 = \"ModelArchive/\"+name+\".h5\"\n",
    "            dstpath1 = \"ModelArchive/\"+ModelName+\".h5\"\n",
    "            shutil.copy(srcpath1,dstpath1)\n",
    "            \n",
    "            srcpath1 = \"TrainedModel/\"+name+\".h5\"\n",
    "            dstpath1 = \"TrainedModel/\"+ModelName+\".h5\"\n",
    "            shutil.copy(srcpath1,dstpath1)\n",
    "            \n",
    "            return cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOeae-r5bl5S"
   },
   "outputs": [],
   "source": [
    "# The objective function\n",
    "\n",
    "def ObjectiveFunction(sol, ModelName, mysolution, patience):\n",
    "    KB.clear_session()\n",
    "    print('**************************** '+ModelName+' ****************************')\n",
    "    result = CheckAndCost(mysolution,sol,ModelName)\n",
    "    if result != 'NotFound':\n",
    "        print('My checking result = '+ str(result))\n",
    "        return result\n",
    "\n",
    "    SOLMAT = Bit2SOLMat(K,sol)\n",
    "    LISTs = Bit2LISTs(sol,BITS_list,SPACE_list, nClass, K)\n",
    "\n",
    "    [FILTERLIST,KERNELIST,MAXPLIST,nNeurons,ACTLIST,OPTIMIZER,LR] = LISTs\n",
    "    early_stopping= tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  mode='min',\n",
    "                                                  patience=patience,\n",
    "                                                  restore_best_weights= True,\n",
    "                                                  )\n",
    "    print('Lists = ')\n",
    "    print(LISTs)\n",
    "    CNNGenerator(SOLMAT,ACTLIST,FILTERLIST,KERNELIST,K,INPUTSHAPE,ModelName,MAXPLIST,nNeurons,RANDOMROT,RANDOMFLIP)\n",
    "    model2 = tf.keras.models.load_model('ModelArchive/'+ModelName+'.h5')\n",
    "#   keras.utils.plot_model(model2, \"arch.png\", show_shapes=True, show_layer_activations=True)\n",
    "    checkpoint_filepath =('TrainedModel/'+ModelName+'.h5') \n",
    "\n",
    "\n",
    "  #  It only saves when the model is considered the \"best\" ( when the val_loss is minumum found so far)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_filepath,\n",
    "                                                                monitor = 'val_accuracy',\n",
    "                                                                mode = 'max',\n",
    "                                                                save_best_only = True)\n",
    "    lr_decayed_fn = LR[0]\n",
    "\n",
    "    if OPTIMIZER[0]=='RMSprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate = lr_decayed_fn)\n",
    "\n",
    "    elif OPTIMIZER[0]=='Adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = lr_decayed_fn)\n",
    "\n",
    "    elif OPTIMIZER[0]=='Adagrad':\n",
    "        optimizer = tf.keras.optimizers.Adagrad(learning_rate = lr_decayed_fn)\n",
    "\n",
    "    elif OPTIMIZER[0]=='Adadelta':\n",
    "        optimizer = tf.keras.optimizers.Adadelta(learning_rate = lr_decayed_fn)\n",
    "\n",
    "    model2.compile(loss = 'categorical_crossentropy', optimizer=optimizer, metrics = ['accuracy'])\n",
    "    \n",
    "    history = model2.fit(datagen.flow(X_shuffled, y_shuffled, batch_size=1), epochs=nEpoch, validation_data=(x_valid, y_valid), shuffle=True,callbacks=[model_checkpoint_callback, early_stopping],verbose=1)\n",
    "\n",
    "    np.save('HistoryArchive/'+ModelName+'.npy',history.history)\n",
    "\n",
    "    bs = - np.max(history.history['val_accuracy'])\n",
    "\n",
    "    cp = model2.count_params()\n",
    "\n",
    "    latency = get_flops(model2, batch_size = 1)\n",
    "    best_score = np.array([bs,cp,latency])\n",
    "    print('Objectives = ')\n",
    "    print(best_score)\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsyxUfQybb1P"
   },
   "outputs": [],
   "source": [
    "# Loading the model's training history\n",
    "\n",
    "def LoadHistory(ModelName):\n",
    "  # PLOT LOSS AND ACCURACY\n",
    "    %matplotlib inline\n",
    "\n",
    "    import matplotlib.image  as mpimg\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    \n",
    "    history=np.load(('HistoryArchive/'+ModelName+'.npy'),allow_pickle='TRUE').item()\n",
    "\n",
    "    acc=history['accuracy']\n",
    "    val_acc=history['val_accuracy']\n",
    "    loss=history['loss']\n",
    "    val_loss=history['val_loss']\n",
    "\n",
    "    epochs=range(len(acc)) # Get number of epochs\n",
    "    return acc, val_acc, loss, val_loss, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions needed for QNSA\n",
    "\n",
    "def Dominates(x,y):\n",
    "  xc = np.array(x['Cost'])\n",
    "  yc = np.array(y['Cost'])\n",
    "\n",
    "  b = (xc <= yc).all() and (xc < yc).any()\n",
    "  return b\n",
    "\n",
    "\n",
    "def NonDominatedSorting(pop):\n",
    "  nPop = len(pop)\n",
    "\n",
    "  for i in range(nPop):\n",
    "    pop[i]['DominationSet'] = []\n",
    "    pop[i]['DominatedCount'] = 0\n",
    "\n",
    "  F = [[]]\n",
    "\n",
    "  for i in range(nPop):\n",
    "    for j in range(i+1,nPop):\n",
    "      p = pop[i].copy()\n",
    "      q = pop[j].copy()\n",
    "\n",
    "      if Dominates(p,q):\n",
    "        p['DominationSet'].append(j)\n",
    "        q['DominatedCount'] += 1\n",
    "      if Dominates(q,p):\n",
    "        q['DominationSet'].append(i)\n",
    "        p['DominatedCount'] += 1\n",
    "\n",
    "      pop[i] = p.copy()\n",
    "      pop[j] = q.copy()\n",
    "\n",
    "    if pop[i]['DominatedCount'] == 0:\n",
    "      F[0].append(i)\n",
    "      pop[i]['Rank'] = 0\n",
    "  k = 0\n",
    "\n",
    "  while True:\n",
    "    Q = []\n",
    "\n",
    "    for i in F[k]:\n",
    "      p = pop[i]\n",
    "      for j in p['DominationSet']:\n",
    "        q = pop[j]\n",
    "        q['DominatedCount'] -=1\n",
    "   \n",
    "        if q['DominatedCount'] == 0: # Only dominated with F[0] population\n",
    "          Q.append(j)\n",
    "          q['Rank'] = k+1\n",
    "\n",
    "        pop[j] = q\n",
    "    if not Q:\n",
    "      break\n",
    "\n",
    "    F.append(Q)\n",
    "    k += 1\n",
    "      \n",
    "  return pop, F\n",
    "\n",
    "def CalcCrowdingDistance(pop,F):\n",
    "  nf = len(F)\n",
    "\n",
    "  for k in range(nf):\n",
    "\n",
    "    Costs = []\n",
    "    for j in F[k]:\n",
    "      Costs.append(pop[j]['Cost'])\n",
    "\n",
    "    Costs = np.array(Costs).transpose()\n",
    "\n",
    "    nobj = Costs.shape[0]\n",
    "    n = len(F[k])\n",
    "    d = np.zeros((n,nobj))\n",
    "    for j in range(nobj):\n",
    "      c = Costs[j,:]\n",
    "      cj = (np.sort(c))\n",
    "      so = (np.argsort(c,kind = 'stable')).astype(int)\n",
    "      d[so[0],j] = np.inf\n",
    "\n",
    "      for i in range(1,n-1):\n",
    "        if cj[i+1] == cj[i-1]:\n",
    "           d[so[i],j] = 0\n",
    "        else:\n",
    "          d[so[i],j] = np.abs(cj[i+1]-cj[i-1]) / (np.abs(cj[0]-cj[-1]))\n",
    "\n",
    "      d[so[-1],j] = np.inf\n",
    "\n",
    "    for i in range(n):\n",
    "      m = F[k][i]\n",
    "      pop[m]['CrowdingDistance'] = np.sum(d[i,:])\n",
    "\n",
    "  return pop\n",
    "\n",
    "def SortPopulation(pop):\n",
    "  CD = []\n",
    "  for i in range(len(pop)):\n",
    "    CD.append(pop[i]['CrowdingDistance'])\n",
    "  CD = np.array(CD)\n",
    "  Idx1 = CD.argsort(kind = 'stable')[::-1].astype(int)\n",
    "  pop = [pop[i] for i in Idx1]\n",
    "\n",
    "  RS = []\n",
    "  for i in range(len(pop)):\n",
    "    RS.append(pop[i]['Rank'])\n",
    "  RS = np.array(RS)\n",
    "  Idx2 = RS.argsort(kind = 'stable').astype(int)\n",
    "  pop = [pop[i] for i in Idx2]\n",
    "\n",
    "  ranks = [p['Rank'] for p in pop]\n",
    "  maxranks = np.max(ranks)\n",
    "\n",
    "  F = []\n",
    "  ranks = np.array(ranks)\n",
    "\n",
    "  if maxranks == 0:\n",
    "    F = [np.arange(len(pop))]\n",
    "  else:\n",
    "    for i in range(maxranks+1):\n",
    "      result = np.where(ranks == i)[0]\n",
    "      F.append(result)\n",
    "  return pop, F\n",
    "\n",
    "def Crossover(x1,x2):\n",
    "  alpha = np.random.random(len(x1))\n",
    "  y11 = np.multiply(alpha,x1)\n",
    "  y12 = np.multiply(1-alpha,x2)\n",
    "  y1 = y11+y12\n",
    "\n",
    "  y21 = np.multiply(alpha,x2)\n",
    "  y22 = np.multiply(1-alpha,x1)\n",
    "  y2 = y21+y22\n",
    "\n",
    "  return y1, y2\n",
    "\n",
    "def Mutate(x, mu, sigma):\n",
    "  nVar = len(x)\n",
    "  nMu = np.ceil(mu*nVar).astype(int)\n",
    "\n",
    "  j = np.random.choice(nVar, nMu, replace = False)\n",
    "  y = x.copy()\n",
    "  for i in j:\n",
    "\n",
    "    y[i] = x[i] + sigma[0][i] * np.random.normal()\n",
    "\n",
    "  return y\n",
    "\n",
    "def Prune(x, Type_List):\n",
    "    n_bin = np.where(Type_List == 0)\n",
    "    n_one = np.where(x == 1)\n",
    "    n_prune = np.intersect1d(n_bin, n_one)\n",
    "    y = x.copy()\n",
    "    print(n_bin)\n",
    "    print(n_one)\n",
    "    print(n_prune)\n",
    "    if len(n_prune)>0:\n",
    "        i = np.random.choice(n_prune)\n",
    "        y[i] = 0\n",
    "    else:\n",
    "        i = np.random.choice(n_bin[0])\n",
    "        y[i] = 1\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crossover operators\n",
    "\n",
    "def Crossover(p1,p2,p3,Q_tensor,Quarter,Type_List):\n",
    "    \n",
    "    # pre alloction of children\n",
    "    c1 = np.zeros_like(p1['Position'])\n",
    "    c2 = np.zeros_like(p1['Position'])\n",
    "\n",
    "    # Sort based on ranks\n",
    "    ranks_list = []\n",
    "    ranks_list.append(p1['Rank'])\n",
    "    ranks_list.append(p2['Rank'])\n",
    "    ranks_list.append(p3['Rank'])\n",
    "    ranks_list = np.array(ranks_list)\n",
    "    ranks_ind = ranks_list.argsort(kind = 'stable').astype(int)\n",
    "\n",
    "    three_pop =[]\n",
    "    three_pop.append(p1.copy())\n",
    "    three_pop.append(p2.copy())\n",
    "    three_pop.append(p3.copy())\n",
    "\n",
    "    \n",
    "    Best = three_pop[ranks_ind[0]]['Position'].copy()\n",
    "    Mid = three_pop[ranks_ind[1]]['Position'].copy()\n",
    "    Worst = three_pop[ranks_ind[2]]['Position'].copy()\n",
    "    \n",
    "    # Choice of actions\n",
    "    ChosenActions = np.zeros(3) # for each type\n",
    "    \n",
    "    for Vartype in range(0,3):\n",
    "        \n",
    "        vars_indx = np.where(Type_List == Vartype)[0]\n",
    "            \n",
    "        qtable = Q_tensor[Quarter][Vartype]\n",
    "        arr_softmax = np.exp(qtable) / np.sum(np.exp(qtable))\n",
    "        # Here we can make some of the zero!\n",
    "        ChosenActions[Vartype] = int(np.random.choice(range(len(qtable)), p = arr_softmax))\n",
    "        print('Var = '+str(Vartype)+', Probabilities'+str(arr_softmax)+', Choice = '+str(ChosenActions[Vartype]))\n",
    "        Best_vars = Best[vars_indx]\n",
    "        Mid_vars = Mid[vars_indx]\n",
    "        Worst_vars = Worst[vars_indx]\n",
    "        \n",
    "        p1_vars = p1['Position'][vars_indx]\n",
    "        p2_vars = p2['Position'][vars_indx]\n",
    "        p3_vars = p3['Position'][vars_indx]\n",
    "\n",
    "        if ChosenActions[Vartype] == 0: # Action 1: One-point crossover\n",
    "            c1_vars,c2_vars = one_point_crossover(Best_vars, Mid_vars)\n",
    " \n",
    "        elif ChosenActions[Vartype] == 1: # Action 2: Uniform crossover\n",
    "            c1_vars,c2_vars = uniform_crossover(Best_vars, Mid_vars)\n",
    "\n",
    "        elif ChosenActions[Vartype] == 2: # Action 3: Jaya crossover\n",
    "            c1_vars,c2_vars = jaya_crossover(Best_vars, Mid_vars, Worst_vars)\n",
    "        \n",
    "        print(c1_vars)\n",
    "        print(c2_vars)\n",
    "        c1[vars_indx] = c1_vars \n",
    "        c2[vars_indx] = c2_vars \n",
    "    return c1, c2, ChosenActions\n",
    "\n",
    "def one_point_crossover(arr1, arr2): # 0\n",
    "    # randomly generate a crossover point\n",
    "    point = np.random.randint(1, len(arr1))\n",
    "    \n",
    "    # perform crossover at the crossover point\n",
    "    new_arr1 = np.concatenate((arr1[:point], arr2[point:]))\n",
    "    new_arr2 = np.concatenate((arr2[:point], arr1[point:]))\n",
    "    \n",
    "    return new_arr1, new_arr2\n",
    "\n",
    " \n",
    "def uniform_crossover(arr1, arr2): # 3\n",
    "    p=0.5\n",
    "    \n",
    "    mask = np.random.choice([True, False], size=len(arr1), p=[p, 1-p])\n",
    "    new_arr1 = np.where(mask, arr1, arr2)\n",
    "    new_arr2 = np.where(mask, arr2, arr1)\n",
    "    return new_arr1, new_arr2\n",
    "\n",
    "def jaya_crossover(parent1, parent2, parent3): # 5\n",
    " \n",
    "    # First child                              \n",
    "    r1 = np.random.random(len(parent2))\n",
    "    d1 = parent1-np.abs(parent2)\n",
    "\n",
    "    r2 = np.random.random(len(parent2))\n",
    "    d2 = parent3-np.abs(parent2)\n",
    "\n",
    "    t1 = np.multiply(r1,d1)\n",
    "    t2 = np.multiply(r2,d2)\n",
    "\n",
    "    offspring1 = np.round(parent2+t1+t2)\n",
    "\n",
    "    # Second child                              \n",
    "    r3 = np.random.random(len(parent2))                             \n",
    "    r4 = np.random.random(len(parent2))\n",
    "\n",
    "    t3 = np.multiply(r3,d1)\n",
    "    t4 = np.multiply(r4,d2)\n",
    "\n",
    "    offspring2 = np.round(parent2+t3+t4)\n",
    "    \n",
    "    return offspring1, offspring2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QNSA\n",
    "\n",
    "problem = {\n",
    "    'CostFunction': ObjectiveFunction,\n",
    "    'nVar': nBits,\n",
    "    'VarMin':bounds[:,0],\n",
    "    'VarMax':bounds[:,1]\n",
    "}\n",
    "def QNSA(problem, nIter , nPop):\n",
    "\n",
    "    # Empty particle\n",
    "    empty_individual = { \n",
    "        'Position': None,\n",
    "        'Cost': None,\n",
    "        'Rank': None,\n",
    "        'DominationSet': None,\n",
    "        'DominatedCount': None,\n",
    "        'CrowdingDistance': None,\n",
    "    }\n",
    "\n",
    "    # Extract info\n",
    "    CostFunction = problem['CostFunction']\n",
    "    nVar = problem['nVar']\n",
    "    VarMin = problem['VarMin']\n",
    "    VarMax = problem['VarMax']\n",
    "\n",
    "    # Initilize global best\n",
    "\n",
    "    gbest = {'Position':None,\n",
    "             'Cost':None}\n",
    "\n",
    "    mysolution = []\n",
    "    pop = []\n",
    "    for i in range(0,nPop):\n",
    "      modelname = 'Model_' + str(-1) + '_' + str(i)\n",
    "      pop.append(empty_individual.copy())\n",
    "      pop[i]['Position'] = np.round(np.random.uniform(VarMin, VarMax, nVar))\n",
    "      pop[i]['Cost']= CostFunction(pop[i]['Position'],modelname,mysolution,patience = 5)\n",
    "      mysolution.append((modelname, -1, i, pop[i]['Position'], pop[i]['Cost']))\n",
    "    # Non-Dominated Sorting\n",
    "    pop, F = NonDominatedSorting(pop)\n",
    "\n",
    "    # Calculate Crowding Distance\n",
    "    pop = CalcCrowdingDistance(pop, F)\n",
    "\n",
    "    # Sort Population\n",
    "    pop, F = SortPopulation(pop)\n",
    "    \n",
    "    # Initialize Q able\n",
    "    Q_tensor = np.zeros((4,3,3)) # 4 quarters or states, 3  variable types, and 7 action types\n",
    "    for it in range(nIter):\n",
    "      patience =5 \n",
    "      # Crossover\n",
    "      hnc = nCrossover/2\n",
    "      popc1 = []\n",
    "      popc2 = []\n",
    "      popc = []\n",
    "      for c in range(int(hnc)):\n",
    "        popc1.append(empty_individual.copy())\n",
    "        popc2.append(empty_individual.copy())\n",
    "\n",
    "      for c in range(int(hnc)):\n",
    "        rnd = random.sample(range(0,nPop),3)\n",
    "\n",
    "        p1 = pop[rnd[0]].copy()\n",
    "        p2 = pop[rnd[1]].copy()\n",
    "        p3 = pop[rnd[2]].copy()\n",
    "        \n",
    "        Quarter = int(4*it/nIter)\n",
    "        popc1[c]['Position'], popc2[c]['Position'], ChosenActions = Crossover(p1,p2,p3,Q_tensor,Quarter,Type_List)\n",
    "\n",
    "        popc1[c]['Position'] = np.round(np.maximum(popc1[c]['Position'],VarMin))\n",
    "        popc1[c]['Position'] = np.round(np.minimum(popc1[c]['Position'],VarMax))\n",
    "\n",
    "        popc2[c]['Position'] = np.round(np.maximum(popc2[c]['Position'],VarMin))\n",
    "        popc2[c]['Position'] = np.round(np.minimum(popc2[c]['Position'],VarMax))\n",
    "        \n",
    "        modelname = 'Model_' + str(it) + '_' + str(2*c)\n",
    "        popc1[c]['Cost']= CostFunction(popc1[c]['Position'],modelname,mysolution,patience)\n",
    "        mysolution.append((modelname, it, 2*c, popc1[c]['Position'], popc1[c]['Cost']))\n",
    "        \n",
    "        modelname = 'Model_' + str(it) + '_' + str(2*c+1)\n",
    "        popc2[c]['Cost']= CostFunction(popc2[c]['Position'],modelname,mysolution,patience)\n",
    "        mysolution.append((modelname, it, 2*c+1, popc2[c]['Position'], popc2[c]['Cost']))\n",
    "        \n",
    "        # Reward\n",
    "        reward = 0\n",
    "        \n",
    "        if Dominates(popc1[0],p1):\n",
    "            reward += 1\n",
    "        if Dominates(popc1[0],p2):\n",
    "            reward += 1\n",
    "        if Dominates(popc1[0],p3):\n",
    "            reward += 1\n",
    "        if Dominates(popc2[0],p1):\n",
    "            reward += 1\n",
    "        if Dominates(popc2[0],p2):\n",
    "            reward += 1\n",
    "        if Dominates(popc2[0],p3):\n",
    "            reward += 1\n",
    "\n",
    "        reward /= 6\n",
    "\n",
    "        # Update Q tensor\n",
    "        for Vartype in range(3):\n",
    "            MaxQ = np.max(Q_tensor[Quarter][Vartype])\n",
    "            ch = int(ChosenActions[Vartype])\n",
    "            CurrentQ = Q_tensor[Quarter][Vartype][ch]\n",
    "            alpha = 1 - 0.9*it/nIter\n",
    "            gamma = 0.8\n",
    "            Q_tensor[Quarter][Vartype][ch] = (1-alpha) * CurrentQ + alpha * (reward + gamma * MaxQ)\n",
    "        print('Q_tensor = ')\n",
    "        print(Q_tensor[Quarter])\n",
    "      popc = popc1 + popc2\n",
    "\n",
    "    # Mutation\n",
    "      popm = []\n",
    "      for m in range(int(nMutation)):\n",
    "        popm.append(empty_individual.copy())\n",
    "\n",
    "      for k in range(int(nMutation)):\n",
    "\n",
    "        i1 = np.random.randint(0,nPop)\n",
    "        p1 = pop[i1].copy()\n",
    "        \n",
    "        if np.random.rand()>0.0:\n",
    "            popm[k]['Position'] = Mutate(p1['Position'], mu, sigma)\n",
    "        else:\n",
    "            popm[k]['Position'] = Prune(p1['Position'], Type_List)\n",
    "\n",
    "        popm[k]['Position'] = np.round(np.maximum(popm[k]['Position'],VarMin))\n",
    "        popm[k]['Position'] = np.round(np.minimum(popm[k]['Position'],VarMax))\n",
    "\n",
    "        modelname = 'Model_' + str(it) + '_' + str(2*hnc+k)\n",
    "        popm[k]['Cost'] = CostFunction(popm[k]['Position'],modelname,mysolution, patience)\n",
    "        mysolution.append((modelname, it, 2*hnc+k, popm[k]['Position'], popm[k]['Cost']))\n",
    "        \n",
    "#     # Merge\n",
    "      pop = pop + popc + popm\n",
    "\n",
    "    # #Non-Dominated Sorting\n",
    "      pop, F = NonDominatedSorting(pop)\n",
    "\n",
    "    # # Calculate Crowding Distance\n",
    "      pop = CalcCrowdingDistance(pop, F)\n",
    "\n",
    "    # # Sort Population\n",
    "      pop, temp = SortPopulation(pop)\n",
    "\n",
    "    # # Truncate\n",
    "\n",
    "      pop = pop[0:nPop]\n",
    "\n",
    "    # # Non-Dominated Sorting\n",
    "      pop, F = NonDominatedSorting(pop)\n",
    "\n",
    "    # # Calculate Crowding Distance\n",
    "      pop = CalcCrowdingDistance(pop, F)\n",
    "\n",
    "    # # Sort Population\n",
    "      pop, F = SortPopulation(pop)\n",
    "\n",
    "    # # Store F1\n",
    "      F1 = []\n",
    "      for f in F[0]:\n",
    "        F1.append(pop[f]) \n",
    "      print('Iteration = ',str(it),'# F1 = ',str(len(F1)))\n",
    "    return F1, pop, mysolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running QNSA  ...')\n",
    "frontiers, pop, mysolution = QNSA(problem, nIter , nPop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
